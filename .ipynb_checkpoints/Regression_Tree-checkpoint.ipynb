{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CART4.5简介\n",
    "\n",
    " 相比ID3算法，CART4.5既可以用于回归问题，也可以用于分类问题。CART是一种二分递归分割的技术，其假设决策树是二叉树，\n",
    " 内部结点特征取值为‘是’和‘否’，左分支为取值是‘是’的分支，右分支是取值为‘否’的分支。在这样的递归的构建决策树的过程中，对回归树而言，采用最小化平方误差作为准则；对于分类树而言，采用基尼指数作为准则。\n",
    " \n",
    "- 读取数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import*\n",
    "\n",
    "def loadDataSet(fileName):      #general function to parse tab -delimited floats\n",
    "    dataMat = []                #assume last column is target value\n",
    "    fr = open(fileName)\n",
    "    for line in fr.readlines():\n",
    "        curLine = line.strip().split('\\t')\n",
    "        fltLine = list(map(float,curLine)) #map all elements to float()\n",
    "        dataMat.append(fltLine)\n",
    "   \n",
    "    return dataMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该函数读取一个以tab为分隔符的文件，然后将每行的内容保存成一组浮点数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "# myDat = loadDataSet('ex0.txt')\n",
    "# myDat = mat(myDat)\n",
    "# print(myDat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binSplitDataSet(dataSet, feature, value):\n",
    "    mat0 = dataSet[nonzero(dataSet[:,feature] > value)[0],:]\n",
    "    mat1 = dataSet[nonzero(dataSet[:,feature] <= value)[0],:]\n",
    "    return mat0,mat1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该函数有3个参数：数据集合、待切分的特征和该特征的某个值。在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回归树的生成算法\n",
    "\n",
    "假设X，Y分别是输入输出变量，并且Y使连续的，给定数据集\n",
    "$$D= \\left \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N) \\right \\}$$\n",
    "一个回归树对应着输入空间(特征空间)的划分以及在划分单元上的输出值，假设已将输入空间划分为M个空间$R_1,R_2,\\cdots,R_M $,并且在每个单元上有固定的输出值$c_M$。\n",
    "\n",
    "当输入空间确定时，可以用平方误差$\\sum\\limits_{x_i\\in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，采用平方误差最小的准测来确定最优值，容易得到，单元$R_m$上的$c_m$的最优值$\\hat{c}_{m}$是$R_m$上所有输入实例$x_i$所对应的输出$y_i$的均值。即：\n",
    "$$\\hat{c}_{m}=ave(y_i\\mid x_i\\in R_m)$$\n",
    "\n",
    "输入：训练数据集D\n",
    "\n",
    "输出：回归树$f(x)$\n",
    "\n",
    "在训练数据集的数据空间中，递归的将每个区域划分为两个子区域并决定每个子区域的输出值，构建二叉决策树；\n",
    "\n",
    "(1)选择最优切分变量j与切分点s，求解：\n",
    "$$\\mathop{min}_{j,s}\\left [\\mathop{min}_{c_1}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\mathop{min}_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2  \\right ]$$\n",
    "遍历变量j,对固定的切分变量j扫描切分点s，选择使上式达到最小的对(j,s)。\n",
    "\n",
    "(2)用选定的对(j,s)划分区域并决定相应的输出值：\n",
    "$$R_1(j,s)=\\left \\{ x\\mid x^{(j)}\\leq s \\right \\}\\hspace{2cm} R_2(j,s)=\\left \\{ x\\mid x^{(j)}> s \\right \\} $$\n",
    "$$\\hat{c}_m=\\frac{1}{N_m}\\sum_{x_i\\in R_m(j,s)}y_i,\\hspace{0.5cm}x\\in R_m,\\hspace{0.5cm}m=1,2$$\n",
    "(3)继续对两个子区域调用(1),(2),直至满足停止条件，\n",
    "\n",
    "(4)将输入空间划分为M个区域$R_1,R_2,\\cdots ,R_M$,生成决策树：\n",
    "$$f(x)=\\sum_{m=1}^{M} \\hat{c}_mI(x\\in R_m)$$\n",
    "- 回归树的切分函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regLeaf(dataSet):#returns the value used for each leaf\n",
    "    return mean(dataSet[:,-1])\n",
    "\n",
    "def regErr(dataSet):\n",
    "    return var(dataSet[:,-1]) * shape(dataSet)[0]\n",
    "\n",
    "\n",
    "def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):\n",
    "    tolS = ops[0]; tolN = ops[1]\n",
    "    #if all the target variables are the same value: quit and return value\n",
    "    if len(set(dataSet[:,-1].T.tolist()[0])) == 1: #exit cond 1\n",
    "        return None, leafType(dataSet)\n",
    "    m,n = shape(dataSet)\n",
    "    #the choice of the best feature is driven by Reduction in RSS error from mean\n",
    "    S = errType(dataSet)\n",
    "    bestS = inf; bestIndex = 0; bestValue = 0\n",
    "    for featIndex in range(n-1):\n",
    "        for splitVal in set(dataSet[:, featIndex].T.A.tolist()[0]):\n",
    "            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\n",
    "            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): continue\n",
    "            newS = errType(mat0) + errType(mat1)\n",
    "            if newS < bestS: \n",
    "                bestIndex = featIndex\n",
    "                bestValue = splitVal\n",
    "                bestS = newS\n",
    "    #if the decrease (S-bestS) is less than a threshold don't do the split\n",
    "    if (S - bestS) < tolS: \n",
    "        return None, leafType(dataSet) #exit cond 2\n",
    "    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n",
    "    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):  #exit cond 3\n",
    "        return None, leafType(dataSet)\n",
    "    return bestIndex,bestValue#returns the best feature to split on\n",
    "                              #and the value used for that split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该函数一开始为ops设置了tolN和tolS,tolS是容许的误差下降值，tolN是切分的最少样本数，用于控制函数停止的时机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1,4)):#assume dataSet is NumPy Mat so we can array filtering\n",
    "    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)#choose the best split\n",
    "    if feat == None: return val #if the splitting hit a stop condition return val\n",
    "    retTree = {}\n",
    "    retTree['spInd'] = feat\n",
    "    retTree['spVal'] = val\n",
    "    lSet, rSet = binSplitDataSet(dataSet, feat, val)\n",
    "    retTree['left'] = createTree(lSet, leafType, errType, ops)\n",
    "    retTree['right'] = createTree(rSet, leafType, errType, ops)\n",
    "    return retTree  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createTree()是一个递归函数，首先通过chooseBestSpli()找到最优的分类特征和分类值，然后通过binSplitDataSet()划分左右子树，最后又在左右子树递归调用createTree()。\n",
    "\n",
    "createTree()有四个参数，leafType给出建立叶子结点的函数，errTyp代表误差计算函数，ops是一个包含着建立树所需其他参数的元组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spInd': 1, 'spVal': 0.39435, 'left': {'spInd': 1, 'spVal': 0.582002, 'left': {'spInd': 1, 'spVal': 0.797583, 'left': 3.9871632, 'right': 2.9836209534883724}, 'right': 1.980035071428571}, 'right': {'spInd': 1, 'spVal': 0.197834, 'left': 1.0289583666666666, 'right': -0.023838155555555553}}\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "myDat = loadDataSet('ex0.txt')\n",
    "myDat = mat(myDat)\n",
    "ret = createTree(myDat)\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是根据'ex0.txt'文件所建立的回归树，节点的标号代表着节点建立的先后顺序，这个顺序和函数的递归有关。\n",
    "<img src=\"Regression_tree-1.png\" style=\"width:600px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 树的剪枝\n",
    "\n",
    "树构建算法其实对输人的参数tolS和tolN敏感，如果使用其他值将不太容易达到这么好的效果。为了说明这一点，\n",
    "在python中输入命令ret = createTree(myDat，ops=(0，1)) 时会构造一个特别臃肿的树，以至于每个样本最终都分配了叶子节点，这是很明显的过拟合情况，但是大多数情况下，我们并不能直接知道ops的最优值。基于这种情况，我们必须采取剪枝算法。\n",
    "\n",
    "- 后剪枝\n",
    "\n",
    "使用后剪枝方法需要将数据集分成测试集和训练集。首先指定参数，使得构建出的树足够大、足够复杂，便于剪枝。接下来从上而下找到叶节点，用测试集来判断将这些叶节点合并是否能降低测试误差。如果是的话就合并。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTree(obj):\n",
    "    return (type(obj).__name__=='dict')\n",
    "\n",
    "def getMean(tree):\n",
    "    if isTree(tree['right']): tree['right'] = getMean(tree['right'])\n",
    "    if isTree(tree['left']): tree['left'] = getMean(tree['left'])\n",
    "    return (tree['left']+tree['right'])/2.0\n",
    "    \n",
    "def prune(tree, testData):\n",
    "    if shape(testData)[0] == 0: return getMean(tree) #if we have no test data collapse the tree\n",
    "    if (isTree(tree['right']) or isTree(tree['left'])):#if the branches are not trees try to prune them\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "    if isTree(tree['left']): tree['left'] = prune(tree['left'], lSet)\n",
    "    if isTree(tree['right']): tree['right'] =  prune(tree['right'], rSet)\n",
    "    #if they are now both leafs, see if we can merge them\n",
    "    if not isTree(tree['left']) and not isTree(tree['right']):\n",
    "        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n",
    "        errorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) +\\\n",
    "            sum(power(rSet[:,-1] - tree['right'],2))\n",
    "        treeMean = (tree['left']+tree['right'])/2.0\n",
    "        errorMerge = sum(power(testData[:,-1] - treeMean,2))\n",
    "        if errorMerge < errorNoMerge: \n",
    "            print(\"merging\")\n",
    "            return treeMean\n",
    "        else: return tree\n",
    "    else: return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪枝算法涉及函数递归，所以具体的细节理解起来特别麻烦，首先在递归调用的过程中，在函数未执行完时，要设置一个标记，以便一层递归执行完后，程序知道从哪开始。另外，函数递归过程中，变量会以栈的形式存储每一次调用所产生的值，再次使用时遵循先进后出的原则，事实上函数的递归调用也遵循栈的原则，即最后被调用要先被执行完，才能继续执行后续的递归调用。\n",
    "\n",
    "我们以上面已经生成的回归树来解释一下，虽然这棵树也许并不过拟合，首先4，5代表着最终的左右子集的平均值，errorNoMerge = sum(power(lSet[:,-1] - tree['left'],2)) +sum(power(rSet[:,-1] - tree['right'],2))代表着算下各自子集的误差，与未分割前3所代表着的子集的误差进行比较，误差理所当然是均方误差。基于两者误差大小的比较，决定是否剪枝。\n",
    "程序进行到这里，也只是完成了最后一次递归调用函数的执行。\n",
    "这里要注意每次递归调用过程中，lSet, rSet，tree['left']，tree['right']，testData都将是不同的，他们以栈的形式存储了很多值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
