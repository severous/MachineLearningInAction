{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 建立一个简单的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceratedata():\n",
    "    dataset = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算信息熵\n",
    "\n",
    "当取自有限的样本时，熵的公式可以表示:\n",
    "$$H(X)=-\\sum_{i} P(x_{i})\\log _{a}P(x_{i})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calShannonEnt(dataset):\n",
    "    numEnt = len(dataset)  # 获得list的长度 即实例总数   注（a）\n",
    "    labelcounts = {}  # 创建一个字典，来存储数据集合中不同label的数量 如 dataset包含3 个‘no’  2个‘yes ’ （用键-值对来存储）\n",
    "    for featVect in dataset:  # 对数据集合的每一个样本进行for遍历\n",
    "        currentlabel = featVect[-1]  # 获得样本的标签\n",
    "        if currentlabel not in labelcounts.keys():  # 如果当前标签在字典键值中不存在\n",
    "            labelcounts[currentlabel] = 0  # 初值\n",
    "        labelcounts[currentlabel] += 1  # 若已经存在该键所对应的值加1\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelcounts:\n",
    "        prob = float(labelcounts[key]) / numEnt\n",
    "        shannonEnt -= prob * math.log(prob, 2)\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdataset(dataset, axis, values):\n",
    "    '''\n",
    "    arguments:\n",
    "        dataset:数据集\n",
    "        axis:数据集的某一列\n",
    "        values:分割数据的树节点\n",
    "    return:\n",
    "        retdataset:剩余数据集\n",
    "        \n",
    "    '''\n",
    "    retdataset = []\n",
    "    for featvect in dataset:\n",
    "#         print('featvect:', featvect)\n",
    "        if featvect[axis] == values:\n",
    "            reducedfeatvec = featvect[:axis]\n",
    "#             print('reducedfeatvec:', reducedfeatvec)\n",
    "            reducedfeatvec.extend(featvect[axis+1:])\n",
    "            retdataset.append(reducedfeatvec)\n",
    "#             print('retdataset:', retdataset)\n",
    "    print('retdataset:', retdataset)\n",
    "    return retdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retdataset: [[1, 'no'], [1, 'no']]\n",
      "retdataset: [[1, 'yes'], [1, 'yes'], [0, 'no']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'yes'], [1, 'yes'], [0, 'no']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "dataset, lables = ceratedata()\n",
    "splitdataset(dataset, 0, 0)\n",
    "splitdataset(dataset, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选择最好的分割点\n",
    "\n",
    "特征选择在于选择对数据有分类能力的特征，这样可以提高决策树的分类能力。通常特征选择的准则时信息增益,在选择特征时\n",
    "，我们对所有特征都进行尝试，在分割完成后，计算所有情况下的信息增益，选取那个信息增益最大的所对应的特征作为树的分割节点。\n",
    "\n",
    "\n",
    "就像我们上面的测试代码所显示的那样，在特征1下我们已经完成了数据分割，计算剩下数据集的信息熵，用它减去未分割前数据集的信息熵，这就是特征1的信息增益。接下来，同样的方式计算特征2的信息熵，两者进行对比即可。就可以得到最好的分割点了。\n",
    "\n",
    "- 信息增益\n",
    "\n",
    "特征集A对训练数据集D的信息增益为$g(D,A)$,定义为集合D的信息熵$H(D)$与特征A在给定条件D的经验条件熵$H(D|A)$之差，即\n",
    "$$g(D,A)=H(D|A)-H(D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataset):\n",
    "    '''\n",
    "    arguments:\n",
    "        dataset:数据集\n",
    "    return：\n",
    "        bestfeature:返回要作为树节点的特征的列索引\n",
    "    '''\n",
    "    numfeatures = len(dataset[0])-1\n",
    "    basentropy = calShannonEnt(dataset)\n",
    "    bestimgain = 0.0\n",
    "    bestfeature = -1\n",
    "    for i in range(numfeatures):\n",
    "        featlist = [example[i] for example in dataset]  # 用于将第i列的元素提取出来\n",
    "        uniquevalu = set(featlist)  # 利用集合的特性删除掉重复的元素，神来之笔。\n",
    "        newentropy = 0.0\n",
    "        for value in uniquevalu:\n",
    "            subdataset = splitdataset(dataset, i, value)\n",
    "            prob = len(subdataset)/float(len(dataset))\n",
    "            newentropy += prob*calShannonEnt(subdataset)\n",
    "        imformationgain = basentropy-newentropy\n",
    "        if(imformationgain>bestimgain):\n",
    "            bestimgain = imformationgain\n",
    "            bestfeature = i\n",
    "    return bestfeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 投票机制\n",
    "\n",
    "如果已经处理完了所有属性，但是类标签依然不是唯一的，此时需要我们决定如何处理该叶子节点，在这种情况下我们一般会采取投票机制。\n",
    "\n",
    "  在若干次迭代后，假设数据集只剩下[[1, 'no'], [1, 'yes'], [0, 'no'], [1, 'yes']]\n",
    "    如果再调用一次splitdataset(dataset, 0, 1)，数据集就只剩下['yes',‘yes’'no'], 需要投票表决选择类标签,返回出现次数最多的那个\n",
    "    (事实上程序采取的算法是存在问题的，要是两个标签出现的次数相同，就会出现误差，其实我们可以采取某种加权的方式来解决这个问题)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorcnt(classlist):\n",
    "    classcount={}\n",
    "    for vote in classlist:\n",
    "        if vote not in classcount.keys():\n",
    "            classcount[vote] = 0\n",
    "        classcount[vote] += 1\n",
    "    sortedclasscount = sorted(classcount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedclasscount[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 建立决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatree(dataset, labels):\n",
    "    \n",
    "    classlist = [example[-1] for example in dataset]  # classlist收集的是数据的最后一列\n",
    "     #  以下应该是树终止的条件\n",
    "    #  在我们把特征1作为最优的分割点后，在特征1为no的这一边，数据集只剩下[[1, 'no'],[1,'n0']]\n",
    "    #   容易看出，类标签只剩下一类，满足下面的条件。\n",
    "    #  count() 方法用于统计字符串里某个字符出现的次数。\n",
    "    \n",
    "    if classlist.count(classlist[0]) == len(classlist):\n",
    "        \n",
    "        return classlist[0]\n",
    "    #  若是类标签并非只剩下一类，这是就要采用投票机制\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorcnt(classlist)\n",
    "    # 在dataset数据集反复的使用chooseBestFeatureToSplit()来选取最好的内部节点\n",
    "    # 因为函数是递归使用的，所以每次进入此函数的dataset是不同的\n",
    "    bestfeat = chooseBestFeatureToSplit(dataset)\n",
    "    bestfeatlabel = labels[bestfeat]  # 读取当前最好分类样本特征的名字\n",
    "    \n",
    "    mytree = {bestfeatlabel: {}}  # python创建树节点的方式\n",
    "    del(labels[bestfeat])  # 删掉已经用来分类点样本特征\n",
    "    featvalues = [example[bestfeat] for example in dataset]\n",
    "    uniquevalue = set(featvalues)\n",
    "    print(dataset)\n",
    "    for value in uniquevalue:\n",
    "        sublabels = labels[:]\n",
    "        mytree[bestfeatlabel][value] = creatree(splitdataset(dataset, bestfeat, value), sublabels)\n",
    "    return mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retdataset: [[1, 'no'], [1, 'no']]\n",
      "retdataset: [[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "retdataset: [[1, 'no']]\n",
      "retdataset: [[1, 'yes'], [1, 'yes'], [0, 'no'], [0, 'no']]\n",
      "[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
      "retdataset: [[1, 'no'], [1, 'no']]\n",
      "retdataset: [[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "retdataset: [['no']]\n",
      "retdataset: [['yes'], ['yes']]\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
      "retdataset: [['no']]\n",
      "retdataset: [['yes'], ['yes']]\n",
      "tree: {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "data, item = ceratedata()\n",
    "tree = creatree(data, item)\n",
    "print('tree:',tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-23-f60605e2bfdb>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-f60605e2bfdb>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def traves_tree(tree):\n",
    "    for key, value in tree.items():\n",
    "        print(key,\":\",value)\n",
    "        \n",
    "    return key,value\n",
    "    \n",
    "# def graphviz_tree(tree,Xlables,Ylables,X):  \n",
    "#     key, value = traves_tree(tree)\n",
    "#     if key == Xlables[:]:\n",
    "#         dot.node('A', key)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
