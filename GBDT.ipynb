{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT的简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建结点类\n",
    "\n",
    "创建结点类的目的是为了存储每次划分的最优特征和阈值。\n",
    "\n",
    "使用类的原因在于在建立树的过程中，需要知道每个结点的具体参数，这样每个结点就是一个类对象。\n",
    "在下面的程序中我们创建的`_build_tree()`函数的返回值就是一个DecisionNode实例对象。由于涉及到函数的递归，最先调用的`_build_tree()`函数将最最后结束并返回该树的根节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i          # Index for the feature that is tested\n",
    "        self.threshold = threshold          # Threshold value for feature\n",
    "        self.value = value                  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch      # 'Left' subtree\n",
    "        self.false_branch = false_branch    # 'Right' subtree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT实现的相关函数\n",
    "### GBDT算法\n",
    "$$Grandient\\quad boosting = Gradient\\quad descent\\quad +\\quad Boosting$$\n",
    "- Adaboost\n",
    "\n",
    "在Adaboost中针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器(强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。使用adaboost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。换句话说，Adaboost通过加大分错样本的权重来克服分类器的不足\n",
    "<img src=\"adaboost.png\" style=\"width:600px;height:150px;\">\n",
    "$$\\text{Adaboost:}\\hspace{0.5cm}H(x)=\\sum_{t}\\rho_th_t(x)$$\n",
    "<img src=\"adaboost2.png\" style=\"width:400px;height:300px;\">\n",
    "\n",
    "- GBDT\n",
    "\n",
    "在理解Adaboost的基础上，GBDT采取的是和Adaboost相同的思路，不同的是GBDT通过梯度来克服分类器的不足。我们可以将其概括如下：\n",
    "\n",
    "输入:训练集$T=\\left \\{\\right(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N) \\}$,损失函数$L(y,f(x))$\n",
    "\n",
    "输出：回归树$\\hat{f(x)}$\n",
    "\n",
    "(1)初始化：$f_0(x)=argmin_{c}\\sum_{i}^{N}L(y_i,f(x))$,当然一般是$f(x)=\\frac{\\sum_{i=1}^{N}y_i}{N}$\n",
    "\n",
    "(2)循环执行m次，直至收敛。\n",
    "\n",
    "(a)计算负梯度：$$-\\frac{\\partial{L(y_i,f(x_i))))}}{\\partial{f(x_i))}}$$\n",
    "当然如果我们的损失函数取的是平方损失函数的话，这时候我们的负梯度恰好就是我们所说的残差$y-y_{pred}$\n",
    "\n",
    "(b)用负梯度取拟合一个回归树$h$\n",
    "\n",
    "(c)更新$f:f+\\rho h$\n",
    "\n",
    "- GBDT相关函数\n",
    "\n",
    "`calculate_gradient`用于计算负梯度，由于我们采取的损失函数是平方损失函数，所以负梯度恰好是残差，并且要注意的是，我们多加了一个负号，但是在更新过程中我们用的是减号，两相抵消，符合(2)(c)\n",
    "\n",
    "在建立回归树的过程中，我们采用的损失函数是平方损失函数，及当我们每次把数据分为两部分时，我们都要计算这两部分数据的最小平方误差，即$\\sum\\limits_{x_i\\in R_m}(y_i-f(x_i))^2$，容易得出当$f(x_i)=mean(y_i)$时，损失函数最小，这时候相当于在计算两部分数据的方差，最后与未分割之前的数据的平方损失进行比较，选取误差减小最大的作为最优的分割特征。最后还要计算误差的减小值，当这个值小于某个阈值的时候这时表示已经没有再分割数据的必要了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, y_pred):\n",
    "    return -(y - y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(X):\n",
    "    \"\"\" Return the variance of the features in dataset X \"\"\"\n",
    "    \n",
    "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
    "    n_samples = np.shape(X)[0]\n",
    "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
    "    \n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_variance_reduction(y, y1, y2):\n",
    "    var_tot = calculate_variance(y)\n",
    "    var_1 = calculate_variance(y1)\n",
    "    var_2 = calculate_variance(y2)\n",
    "    \n",
    "\n",
    "    # Calculate the variance reduction\n",
    "    variance_reduction = var_tot - ((len(y1)/len(y))*var_1 +(len(y2)/len(y))*var_2)\n",
    "    \n",
    "    return sum(variance_reduction) # sum是为了把只有一个元素的一维数组中的数据提取出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立树结点\n",
    "\n",
    "`divide_on_feature`用于根据阈值划分数据，`_mean_of_y`用于当停止分割后，计算每个叶子结点的值，即树的预测值。\n",
    "`_build_tree`采用递归方法建立树，该函数的返回值是DecisionNode实例对象，这样做的好处，每个树结点都是一个实例对象\n",
    "，可以通过调用其属性，知道该结点的特征，尤其要注意的是，属性true_branc和false_branch也是DecisionNode实例对象，这相当于一个嵌套，我们可以通过这个嵌套来遍历整棵树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
    "        the given threshold \"\"\"\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mean_of_y(y):\n",
    "    value = np.mean(y, axis=0)\n",
    "    return value if len(value) > 1 else value[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_tree(X, y,current_depth=0, min_samples_split=2,  max_depth=4, min_impurity=1e-7):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "       \n",
    "        largest_impurity = 0\n",
    "        best_criteria = None  # Feature index and threshold\n",
    "        best_sets = None  # Subsets of the data\n",
    "\n",
    "    # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "    # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        if n_samples >= min_samples_split and current_depth <= max_depth:\n",
    "        # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "        # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "            # Iterate through all unique values of feature column i and\n",
    "            # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "            # Divide X and y depending on if the feature value of X at index feature_i\n",
    "            # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "                    # Calculate impurity\n",
    "                        impurity = _calculate_variance_reduction(y, y1, y2)\n",
    "                # If this threshold resulted in a higher information gain than previously\n",
    "                # recorded save the threshold value and the feature\n",
    "                # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
    "                                }\n",
    "        if largest_impurity > min_impurity:\n",
    "        # Build subtrees for the right and left branches\n",
    "            true_branch =  _build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth+ 1)\n",
    "            false_branch = _build_tree(best_sets[\"rightX\"], best_sets[\"righty\"],current_depth+ 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "    # We're at leaf => determine value\n",
    "        leaf_value = _mean_of_y(y)\n",
    "        return DecisionNode(value=leaf_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测\n",
    "\n",
    "`predict_value`实在不断递归调用DecisionNode实例对象，来实现遍历整个回归树，每一个树结点相当于对数据进行划分，直至遇到叶子结点，即返回预测值。\n",
    "\n",
    "`predict`一个一个样本的调用`predict_value`,这样就不会破坏原有的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_value(x, root):\n",
    "    \n",
    "    tree = root\n",
    "    if tree.value is not None:\n",
    "        return tree.value\n",
    "    feature_value = x[tree.feature_i]\n",
    "    branch = tree.false_branch\n",
    "    \n",
    "    if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "        if feature_value >= tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "                \n",
    "    elif feature_value == tree.threshold:\n",
    "        branch = tree.true_branch\n",
    "\n",
    "    return predict_value(x, branch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,root):\n",
    "    y_pred = [predict_value(sample,root) for sample in X]\n",
    "    return y_pred    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y,n_estimators=200,learning_rate=0.5):\n",
    "    y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
    "  \n",
    "    for i in range(n_estimators):\n",
    "        gradient = calculate_gradient(y, y_pred)\n",
    "        \n",
    "        root= _build_tree(X, gradient)\n",
    "        \n",
    "        \n",
    "        update = predict(X, root)\n",
    "        \n",
    "            # Update y prediction\n",
    "        y_pred -= np.multiply(learning_rate, update)\n",
    "         \n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, y, seed=None):\n",
    "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    idx = np.arange(X.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
    "    \"\"\" Split the data into train and test sets \"\"\"\n",
    "    if shuffle:\n",
    "        X, y = shuffle_data(X, y, seed)\n",
    "    # Split the training data from test data in the ratio specified in\n",
    "    # test_size\n",
    "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
    "    X_train, X_test = X[:split_i], X[split_i:]\n",
    "    y_train, y_test = y[:split_i], y[split_i:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Gradient Boosting Regression --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\123456\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n",
      "C:\\Users\\123456\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c5d2e57208>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\"-- Gradient Boosting Regression --\")\n",
    "\n",
    "# Load temperature data\n",
    "data = pd.read_csv('F:\\python\\GBDT\\ML-From-Scratch-master\\ML-From-Scratch-master\\mlfromscratch\\data\\TempLinkoping2016.txt', sep=\"\\t\")\n",
    "# as_matrix(): Convert the frame to its Numpy-array representation.\n",
    "    # np.atleast_2d:View inputs as arrays with at least two dimensions.\n",
    "time = np.atleast_2d(data[\"time\"].as_matrix()).T\n",
    "temp = np.atleast_2d(data[\"temp\"].as_matrix()).T\n",
    "\n",
    "X = time.reshape((-1, 1))               # Time. Fraction of the year [0, 1]\n",
    "X = np.insert(X, 0, values=1, axis=1)   # Insert bias term\n",
    "\n",
    "y = temp[:, 0]                          # Temperature. Reduce to one-dim\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "y_pred = fit(X_train, y_train)\n",
    "plt.scatter(X_train[:,1],y_train)\n",
    "plt.scatter(X_train[:,1],y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
